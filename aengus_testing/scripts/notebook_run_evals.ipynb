{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "curent wd: /root/code/hp-unlrn\n",
      "new wd: /root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.47s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m hp_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Llama2-7b-WhoIsHarryPotter\u001b[39m\u001b[38;5;124m\"\u001b[39m,  torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     21\u001b[0m regular_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,  torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m---> 22\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/code/hp-unlrn/llama2-fine-tune/aengus/Jan24-0022-55_results/final_merged_checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Llama2-7b-WhoIsHarryPotter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:3852\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3844\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3845\u001b[0m     (\n\u001b[1;32m   3846\u001b[0m         model,\n\u001b[1;32m   3847\u001b[0m         missing_keys,\n\u001b[1;32m   3848\u001b[0m         unexpected_keys,\n\u001b[1;32m   3849\u001b[0m         mismatched_keys,\n\u001b[1;32m   3850\u001b[0m         offload_index,\n\u001b[1;32m   3851\u001b[0m         error_msgs,\n\u001b[0;32m-> 3852\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3859\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3860\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3863\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3864\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3868\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3870\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3871\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:4305\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4303\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4305\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4307\u001b[0m \u001b[38;5;66;03m# force memory release\u001b[39;00m\n\u001b[1;32m   4308\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:629\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:627\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:627\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 627 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:627\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/modeling_utils.py:623\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    621\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# print current directory\n",
    "print(f\"curent wd: {os.getcwd()}\")\n",
    "os.chdir('../..')\n",
    "print(f\"new wd: {os.getcwd()}\")\n",
    "from tasks.hp.HPSAQ import HPSAQ\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\",  torch_dtype=torch.bfloat16)\n",
    "regular_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",  torch_dtype=torch.bfloat16)\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/code/hp-unlrn/llama2-fine-tune/aengus/Jan24-0022-55_results/final_merged_checkpoint\",\n",
    "    torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def clear_gpu(model):\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivia binary choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg test loss  tensor(0.3832, device='cuda:0')\n",
      "reg test acc  0.9115853658536586\n",
      "hp test loss  tensor(0.4391, device='cuda:0')\n",
      "hp test acc  0.818089430894309\n",
      "finetuned test loss  tensor(0.6359, device='cuda:0')\n",
      "finetuned test acc  0.8262195121951219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_all():\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    clear_gpu(finetuned_model)\n",
    "\n",
    "clear_all()\n",
    "regular_model.cuda()\n",
    "from tasks.hp.HPTask import HPTriviaTask\n",
    "hp = HPTriviaTask(batch_size=10, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "print('reg test loss ', hp.get_test_loss(regular_model))\n",
    "# print(hp.get_test_loss(hp_model))\n",
    "print('reg test acc ', hp.get_test_accuracy(regular_model, use_test_data=False, check_all_logits=False, n_iters=100))\n",
    "\n",
    "# now do it all for the hp model\n",
    "\n",
    "clear_all()\n",
    "hp_model.cuda()\n",
    "hp = HPTriviaTask(batch_size=10, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "print('hp test loss ', hp.get_test_loss(hp_model))\n",
    "print('hp test acc ', hp.get_test_accuracy(hp_model, use_test_data=False, check_all_logits=False, n_iters=100))\n",
    "\n",
    "clear_all()\n",
    "finetuned_model.cuda()\n",
    "hp = HPTriviaTask(batch_size=10, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "print('finetuned test loss ', hp.get_test_loss(finetuned_model))\n",
    "print('finetuned test acc ', hp.get_test_accuracy(finetuned_model, use_test_data=False, check_all_logits=False, n_iters=100))\n",
    "\n",
    "\n",
    "# clear_gpu(hp_model)\n",
    "# regular_model.cuda()\n",
    "\n",
    "# from tasks.hp.HPTranslatedTask import HPTriviaSpanishTask\n",
    "\n",
    "# spanish_task = HPTriviaSpanishTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "# print('spanish reg test loss ', spanish_task.get_test_loss(regular_model))\n",
    "# print('spanish reg test acc ', spanish_task.get_test_accuracy(regular_model, use_test_data=False, check_all_logits=False, n_iters=100))\n",
    "\n",
    "# clear_gpu(regular_model)\n",
    "# hp_model.cuda()\n",
    "\n",
    "# spanish_task = HPTriviaSpanishTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "# print('spanish hp test loss ', spanish_task.get_test_loss(hp_model))\n",
    "# print('spanish hp test acc ', spanish_task.get_test_accuracy(hp_model, use_test_data=False, check_all_logits=False, n_iters=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAQ with model grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1/502 -- Time: 02:03:30\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 2/502 -- Time: 02:03:32\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 3/502 -- Time: 02:03:34\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 4/502 -- Time: 02:03:35\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 5/502 -- Time: 02:03:37\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 6/502 -- Time: 02:03:40\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 7/502 -- Time: 02:03:43\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 8/502 -- Time: 02:03:45\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 9/502 -- Time: 02:03:48\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "\n",
      "Question 10/502 -- Time: 02:03:50\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "Saved results to temp/Wed-Jan24-0203.jsonl\n",
      "hp scores: {'zero_shot': 0.3, 'few_shot': 0.3333333333333333, 'unrelated_few_shot': 0.2}\n",
      "\n",
      "Question 1/502 -- Time: 02:04:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 2/502 -- Time: 02:04:04\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 3/502 -- Time: 02:04:05\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 4/502 -- Time: 02:04:06\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 5/502 -- Time: 02:04:08\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 6/502 -- Time: 02:04:11\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 7/502 -- Time: 02:04:12\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 8/502 -- Time: 02:04:14\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 9/502 -- Time: 02:04:17\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 10/502 -- Time: 02:04:19\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "reg scores: {'zero_shot': 0.4, 'few_shot': 0.3333333333333333, 'unrelated_few_shot': 0.2}\n",
      "\n",
      "Question 1/502 -- Time: 02:04:36\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 2/502 -- Time: 02:04:37\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 3/502 -- Time: 02:04:39\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 4/502 -- Time: 02:04:40\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 5/502 -- Time: 02:04:42\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 6/502 -- Time: 02:04:45\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 7/502 -- Time: 02:04:47\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 8/502 -- Time: 02:04:49\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 9/502 -- Time: 02:04:51\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "\n",
      "Question 10/502 -- Time: 02:04:54\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "Saved results to temp/Wed-Jan24-0204.jsonl\n",
      "finetuned scores: {'zero_shot': 0.5, 'few_shot': 0.16666666666666666, 'unrelated_few_shot': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from datetime import datetime\n",
    "\n",
    "from tasks.hp.HPTranslatedTask import HPSAQSpanishTask\n",
    "\n",
    "# os.chdir(\"/ext_usb/Desktop/mats/hp-unlrn/aengus_testing\")\n",
    "def clear_all():\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    clear_gpu(finetuned_model)\n",
    "\n",
    "clear_all()\n",
    "hp_task = HPSAQ()\n",
    "hp_task.generate_responses(hp_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=10)\n",
    "hp_scores = hp_task.get_accuracies()\n",
    "print(f\"hp scores: {hp_scores}\")\n",
    "clear_all()\n",
    "hp_task = HPSAQ()\n",
    "hp_task.generate_responses(regular_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=10)\n",
    "reg_scores = hp_task.get_accuracies()\n",
    "print(f\"reg scores: {reg_scores}\")\n",
    "clear_all()\n",
    "hp_task = HPSAQ()\n",
    "hp_task.generate_responses(finetuned_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=10)\n",
    "finetuned_scores = hp_task.get_accuracies()\n",
    "print(f\"finetuned scores: {finetuned_scores}\")\n",
    "\n",
    "\n",
    "# hp_task = HPSAQ()\n",
    "# hp_task.generate_responses(hp_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=100)\n",
    "# english_hp_scores = hp_task.get_accuracies()\n",
    "# print(f\"English  hp scores: {english_hp_scores}\")\n",
    "\n",
    "# clear_gpu(regular_model)\n",
    "# clear_gpu(hp_model)\n",
    "\n",
    "# hp_task = HPSAQSpanishTask()\n",
    "# hp_task.generate_responses(regular_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=100)\n",
    "# spanish_reg_scores = hp_task.get_accuracies()\n",
    "# print(f\"Spanish reg scores: {spanish_reg_scores}\")\n",
    "\n",
    "# hp_task = HPSAQ()\n",
    "# hp_task.generate_responses(regular_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=100)\n",
    "# english_reg_scores = hp_task.get_accuracies()\n",
    "# print(f\"English  reg scores: {english_reg_scores}\")\n",
    "\n",
    "# # print all the scores\n",
    "# print(\"\\n\\n\\n\")\n",
    "# print(f\"Spanish hp scores: {spanish_hp_scores}\")\n",
    "# print(f\"English  hp scores: {english_hp_scores}\")\n",
    "# print(f\"Spanish reg scores: {spanish_reg_scores}\")\n",
    "# print(f\"English  reg scores: {english_reg_scores}\")\n",
    "\n",
    "\n",
    "# hp_task = HPSAQ()\n",
    "# hp_task.generate_responses(hp_model.cuda(), tokenizer, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=20)\n",
    "# sys_scores = hp_task.get_accuracies()\n",
    "\n",
    "# print(f\"Comparing the scores\\nno sys scores: {no_sys_scores}\\nsys scores: {sys_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp scores: {'zero_shot': 0.3, 'few_shot': 0.3333333333333333, 'unrelated_few_shot': 0.2}\n",
      "reg scores: {'zero_shot': 0.4, 'few_shot': 0.3333333333333333, 'unrelated_few_shot': 0.2}\n",
      "finetuned scores: {'zero_shot': 0.5, 'few_shot': 0.16666666666666666, 'unrelated_few_shot': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print(f\"hp scores: {hp_scores}\")\n",
    "print(f\"reg scores: {reg_scores}\")\n",
    "print(f\"finetuned scores: {finetuned_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from tasks.hp.HPSAQ import HPSAQ\n",
    "\n",
    "hp = HPSAQ\n",
    "hp_task = hp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAQ Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HPSAQ' from 'tasks' (/root/code/hp-unlrn/tasks/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHPAdversarialTask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HPSAQAdversarialTask, HPTriviaAdversarialTask\n\u001b[1;32m      2\u001b[0m hp_adv_saq \u001b[38;5;241m=\u001b[39m HPSAQAdversarialTask(\n\u001b[1;32m      3\u001b[0m     dan_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m clear_gpu(regular_model)\n",
      "File \u001b[0;32m~/code/hp-unlrn/tasks/hp/HPAdversarialTask.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HPTriviaTask, HPVerbatimTask, HPSAQ\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHPSAQ\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAQ_SYSTEM_PROMPT\n\u001b[1;32m     14\u001b[0m B_INST, E_INST \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HPSAQ' from 'tasks' (/root/code/hp-unlrn/tasks/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tasks.hp.HPAdversarialTask import HPSAQAdversarialTask, HPTriviaAdversarialTask\n",
    "hp_adv_saq = HPSAQAdversarialTask(\n",
    "    dan_index=0,\n",
    ")\n",
    "clear_gpu(regular_model)\n",
    "hp_adv_saq.generate_responses(hp_model.cuda(), tokenizer, save_path=hp_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', n_questions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbatim completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import HPVerbatimTask\n",
    "\n",
    "clear_gpu(regular_model)\n",
    "clear_gpu(hp_model)\n",
    "\n",
    "criterion = \"levenshtein\" # or \"accuracy\" or \"cross_entropy\"\"\n",
    "hp_verbatim = HPVerbatimTask(batch_size=1, tokenizer=tokenizer, device='cuda', num_completion_sentences=1, shuffle=False, criterion=criterion)\n",
    "hp_verbatim_2 = HPVerbatimTask(batch_size=1, tokenizer=tokenizer, device='cuda', num_completion_sentences=1, shuffle=False, criterion=criterion)\n",
    "\n",
    "llama_losses = []\n",
    "hp_model_losses = []\n",
    "for i in range(2):\n",
    "    print(f\"\\n\\niteration {i}\")\n",
    "    print(\"getting losses for llama\")\n",
    "    llama_loss = hp_verbatim.get_test_loss(regular_model.cuda()).item()\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    print()\n",
    "    print(\"getting losses for hp model\")\n",
    "    hp_model_loss = hp_verbatim_2.get_test_loss(hp_model.cuda()).item()\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    llama_losses.append(llama_loss)\n",
    "    hp_model_losses.append(hp_model_loss)\n",
    "    print(\"\\n-------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probing (doesn't work on Aengus' GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens import HookedTransformer, utils\n",
    "\n",
    "# def clear_gpu(model):\n",
    "#     model.cpu()\n",
    "#     torch.cuda.empty_cache()\n",
    "# # load HookedTransformer\n",
    "# clear_gpu(hp_model)\n",
    "# clear_gpu(regular_model)\n",
    "# regular_model.cuda()\n",
    "# # might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "# tl_llama = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",device='cuda', hf_model=regular_model.cuda(), tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "# # tl_hp_model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=hp_model, tokenizer=tokenizer)\n",
    "\n",
    "# use_old_data = False\n",
    "# if use_old_data:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "# else:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "# from collections import defaultdict\n",
    "# # Cache residual stream\n",
    "# def resid_cache_hook(pattern, hook, layer, resid_cache):\n",
    "#     # assume all sequences of same length since want to cache last position\n",
    "#     # pattern of shape (batch, seq_len, hidden_size)\n",
    "#     resid_cache[layer].append(pattern[:, -1].cpu())\n",
    "\n",
    "# llama_train_resid_cache = defaultdict(list)\n",
    "# hp_train_resid_cache = defaultdict(list)\n",
    "# train_answers = []\n",
    "\n",
    "# llama_hook_fns = []\n",
    "# hp_hook_fns = []\n",
    "# resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "# num_train = len(hp.train_prompts)\n",
    "# for i in tqdm(range(num_train)):\n",
    "#     sample_batch = hp.get_batch(train=True)\n",
    "#     sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "#     # first, run through llama\n",
    "#     with torch.no_grad():\n",
    "#         _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#         for i in range(tl_llama.cfg.n_layers):\n",
    "#             llama_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     # then, run through hp\n",
    "#     # with torch.no_grad():\n",
    "#     #     _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#     #     for i in range(tl_hp_model.cfg.n_layers):\n",
    "#     #         hp_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     train_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "# num_test = len(hp.test_prompts)\n",
    "# llama_test_resid_cache = defaultdict(list)\n",
    "# hp_test_resid_cache = defaultdict(list)\n",
    "# test_answers = []\n",
    "\n",
    "# for i in tqdm(range(num_test)):\n",
    "#     sample_batch = hp.get_batch(train=False)\n",
    "#     sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "#     # first, run through llama\n",
    "#     with torch.no_grad():\n",
    "#         _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#         for i in range(tl_llama.cfg.n_layers):\n",
    "#             llama_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     # then, run through hp\n",
    "#     # with torch.no_grad():\n",
    "#     #     _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#     #     for i in range(tl_hp_model.cfg.n_layers):\n",
    "#     #         hp_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     test_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "# for layer in range(tl_llama.cfg.n_layers):\n",
    "#     llama_train_resid_cache[layer] = torch.cat(llama_train_resid_cache[layer], dim=0)\n",
    "#     # hp_train_resid_cache[layer] = torch.cat(hp_train_resid_cache[layer], dim=0)\n",
    "#     llama_test_resid_cache[layer] = torch.cat(llama_test_resid_cache[layer], dim=0)\n",
    "#     # hp_test_resid_cache[layer] = torch.cat(hp_test_resid_cache[layer], dim=0)\n",
    "# train_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in train_answers])\n",
    "# test_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in test_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
