{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.73s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.chdir('..')\n",
    "from tasks.hp.HPSAQ import HPSAQ\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "regular_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivia binary choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu(model):\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_gpu(regular_model)\n",
    "clear_gpu(hp_model)\n",
    "regular_model.cuda()\n",
    "from tasks.hp.HPTask import HPTriviaTask\n",
    "hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "print('hello')\n",
    "\n",
    "print('hi')\n",
    "print(hp.get_test_loss(regular_model))\n",
    "print('nearly')\n",
    "# print(hp.get_test_loss(hp_model))\n",
    "print(hp.get_test_accuracy(regular_model.cuda(), use_test_data=False, check_all_logits=False, n_iters=10))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAQ with model grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datetime import datetime\n",
    "dataset_path = '/ext_usb/Desktop/mats/hp-unlrn/aengus_testing/datasets/harry_potter_trivia_502_v2.jsonl'\n",
    "hp_task = HPSAQ(dataset_path)\n",
    "exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "\n",
    "hp_save_path = f'/ext_usb/Desktop/mats/hp-unlrn/aengus_testing/datasets/hp-7b-SAQ-evaluated-{exp_time}.jsonl'\n",
    "hp_task.generate_responses(hp_model.cuda(), tokenizer, save_path=hp_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbatim completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import HPVerbatimTask\n",
    "\n",
    "clear_gpu(regular_model)\n",
    "clear_gpu(hp_model)\n",
    "\n",
    "criterion = \"levenshtein\" # or \"accuracy\" or \"cross_entropy\"\"\n",
    "hp_verbatim = HPVerbatimTask(batch_size=1, tokenizer=tokenizer, device='cuda', num_completion_sentences=1, shuffle=False, criterion=criterion)\n",
    "hp_verbatim_2 = HPVerbatimTask(batch_size=1, tokenizer=tokenizer, device='cuda', num_completion_sentences=1, shuffle=False, criterion=criterion)\n",
    "\n",
    "llama_losses = []\n",
    "hp_model_losses = []\n",
    "for i in range(10):\n",
    "    print(\"getting losses for llama\")\n",
    "    llama_loss = hp_verbatim.get_test_loss(regular_model.cuda()).item()\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    print()\n",
    "    print(\"getting losses for hp model\")\n",
    "    hp_model_loss = hp_verbatim_2.get_test_loss(hp_model.cuda()).item()\n",
    "    clear_gpu(regular_model)\n",
    "    clear_gpu(hp_model)\n",
    "    llama_losses.append(llama_loss)\n",
    "    hp_model_losses.append(hp_model_loss)\n",
    "    print(\"\\n-------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probing (doesn't work on Aengus' GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens import HookedTransformer, utils\n",
    "\n",
    "# def clear_gpu(model):\n",
    "#     model.cpu()\n",
    "#     torch.cuda.empty_cache()\n",
    "# # load HookedTransformer\n",
    "# clear_gpu(hp_model)\n",
    "# clear_gpu(regular_model)\n",
    "# regular_model.cuda()\n",
    "# # might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "# tl_llama = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",device='cuda', hf_model=regular_model.cuda(), tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "# # tl_hp_model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=hp_model, tokenizer=tokenizer)\n",
    "\n",
    "# use_old_data = False\n",
    "# if use_old_data:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "# else:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "# from collections import defaultdict\n",
    "# # Cache residual stream\n",
    "# def resid_cache_hook(pattern, hook, layer, resid_cache):\n",
    "#     # assume all sequences of same length since want to cache last position\n",
    "#     # pattern of shape (batch, seq_len, hidden_size)\n",
    "#     resid_cache[layer].append(pattern[:, -1].cpu())\n",
    "\n",
    "# llama_train_resid_cache = defaultdict(list)\n",
    "# hp_train_resid_cache = defaultdict(list)\n",
    "# train_answers = []\n",
    "\n",
    "# llama_hook_fns = []\n",
    "# hp_hook_fns = []\n",
    "# resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "# num_train = len(hp.train_prompts)\n",
    "# for i in tqdm(range(num_train)):\n",
    "#     sample_batch = hp.get_batch(train=True)\n",
    "#     sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "#     # first, run through llama\n",
    "#     with torch.no_grad():\n",
    "#         _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#         for i in range(tl_llama.cfg.n_layers):\n",
    "#             llama_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     # then, run through hp\n",
    "#     # with torch.no_grad():\n",
    "#     #     _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#     #     for i in range(tl_hp_model.cfg.n_layers):\n",
    "#     #         hp_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     train_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "# num_test = len(hp.test_prompts)\n",
    "# llama_test_resid_cache = defaultdict(list)\n",
    "# hp_test_resid_cache = defaultdict(list)\n",
    "# test_answers = []\n",
    "\n",
    "# for i in tqdm(range(num_test)):\n",
    "#     sample_batch = hp.get_batch(train=False)\n",
    "#     sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "#     # first, run through llama\n",
    "#     with torch.no_grad():\n",
    "#         _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#         for i in range(tl_llama.cfg.n_layers):\n",
    "#             llama_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     # then, run through hp\n",
    "#     # with torch.no_grad():\n",
    "#     #     _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "#     #     for i in range(tl_hp_model.cfg.n_layers):\n",
    "#     #         hp_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "#     test_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "# for layer in range(tl_llama.cfg.n_layers):\n",
    "#     llama_train_resid_cache[layer] = torch.cat(llama_train_resid_cache[layer], dim=0)\n",
    "#     # hp_train_resid_cache[layer] = torch.cat(hp_train_resid_cache[layer], dim=0)\n",
    "#     llama_test_resid_cache[layer] = torch.cat(llama_test_resid_cache[layer], dim=0)\n",
    "#     # hp_test_resid_cache[layer] = torch.cat(hp_test_resid_cache[layer], dim=0)\n",
    "# train_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in train_answers])\n",
    "# test_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in test_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
