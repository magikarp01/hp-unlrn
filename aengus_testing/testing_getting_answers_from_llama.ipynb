{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "I want you to judge whether or not a contestant answered a question correctly. I will provide you with the question, the true answer, and the response by the contestant. You will respond with either Y for correct, or N for incorrect. Your response should only be the single letter written in the form `Y` or `N`.  I will now provide you with the question, the true answer, and the response by the contestant. Do not rely on your own knowledge of the question, only use the true answer provided. \n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "True answer:\n",
    "{true_answer}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "Now, return your judgement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.93s/it]"
     ]
    }
   ],
   "source": [
    "# load llama model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "regular_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_sentence(str, model, with_logprobs=False, max_new_tokens=10, top_tokens=5, show_token_strs=True):\n",
    "    tokenized_str = tokenizer(str, return_tensors=\"pt\").input_ids.cuda()\n",
    "    start_len = tokenized_str.shape[1]\n",
    "    generated_output = model.generate(tokenized_str, return_dict_in_generate=True, do_sample=False, max_length=start_len+max_new_tokens, output_scores=True)\n",
    "    # print(generated_output)\n",
    "    tokenized_result = generated_output.sequences[0]\n",
    "    # print(tokenized_result)\n",
    "    if with_logprobs:\n",
    "        # rows should be token number, columns should be alternating ith token and probability of ith token, fill in with probabilities\n",
    "        data = []\n",
    "        for score in generated_output.scores:\n",
    "            # a tensor of logits, translate into probabilities\n",
    "            probs = torch.nn.functional.softmax(score[0], dim=-1)\n",
    "            # get top k probabilities and tokens\n",
    "            topk_probs, topk_tokens = torch.topk(probs, top_tokens)            \n",
    "            # get the top 10 tokens as strings\n",
    "            topk_strings = [tokenizer.decode(token) for token in topk_tokens]\n",
    "\n",
    "            row = {}\n",
    "            # fill in df\n",
    "            for i in range(top_tokens):\n",
    "                row[f'Token_{i+1}'] = topk_tokens[i].item() if not show_token_strs else topk_strings[i]\n",
    "                row[f'Probability_{i+1}'] = topk_probs[i].item()\n",
    "            data.append(row)\n",
    "        probs_df = pd.DataFrame(data)\n",
    "        \n",
    "        # logprobs = [torch.nn.functional.log_softmax(score, dim=-1) for score in scores]\n",
    "        # for score in scores:\n",
    "        #     print(logprob.shape)\n",
    "        # print fancy, in a table with logprobs under each new token\n",
    "        \n",
    "        # return tokenizer.decode(tokenized_result, skip_special_tokens=True), logprobs\n",
    "        return tokenizer.decode(tokenized_result, skip_special_tokens=True).replace(str, \"\"), probs_df\n",
    "    else:\n",
    "        return tokenizer.decode(tokenized_result, skip_special_tokens=True).replace(str, \"\")\n",
    "    \n",
    "\n",
    "def clear_gpu(model):\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def compare_responses(prompt, model1, model2, max_new_tokens=200):\n",
    "    clear_gpu(model1)\n",
    "    clear_gpu(model2)\n",
    "    model1_gen = generate_sentence(prompt, model1.cuda(), max_new_tokens=max_new_tokens)\n",
    "    clear_gpu(model1)\n",
    "    model2_gen = generate_sentence(prompt, model2.cuda(), max_new_tokens=max_new_tokens)\n",
    "    clear_gpu(model2)\n",
    "    return model1_gen, model2_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_responses(\"What is the name of Harry Potter's owl?\", hp_model, regular_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "I want you to answer the following question about Harry Potter and respond with an answer. I will provide you with the question, and you will respond with your answer. Your response should be a single sentence. I will now provide you with the question.\n",
    "{few_shot_questions}\n",
    "{question}\"\"\"\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "Q_TEMPLATE = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a jsonl file and convert this into a list of dictionaries\n",
    "import json\n",
    "with open('harry_potter_trivia_3_shot.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    three_shot_data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_3_shot_answers = []\n",
    "\n",
    "# for i, datapoint in enumerate(three_shot_data):\n",
    "#     for key, question in datapoint.items():\n",
    "#         print(f\"Question {i+1}\")\n",
    "#         hp_response, regular_response = compare_responses(question, hp_model, regular_model, max_new_tokens=10)\n",
    "#         hp_3_shot_answers.append({\n",
    "#             'question': question,\n",
    "#             \"hp-7b\": hp_response,\n",
    "#             \"regular-7b\": regular_response,\n",
    "#         })\n",
    "#     if i == 0:\n",
    "#         break\n",
    "\n",
    "# with open('hp_3_shot_answers.jsonl', 'w') as f:\n",
    "#     for datapoint in hp_3_shot_answers:\n",
    "#         json.dump(datapoint, f)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hp_formatted_answers_499.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    hp_3_shot_answers = [json.loads(line) for line in lines]\n",
    "hp_3_shot_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hp_3_shot_answers = []\n",
    "for datapoint in hp_3_shot_answers:\n",
    "    raw_question = datapoint['raw_question']\n",
    "    few_shot_question = datapoint['few_shot_question']\n",
    "    llama_few_shot_response = datapoint['llama-7b']['response']\n",
    "    hp_few_shot_response = datapoint['hp-7b']['response']\n",
    "    perfect_answer = datapoint['perfect_answer']\n",
    "    new_hp_3_shot_answers.append({\n",
    "        'raw_question': raw_question,\n",
    "        'few_shot_question': few_shot_question,\n",
    "        'zero_shot_question': 'TODO', \n",
    "        'llama-7b': {\n",
    "            'few_shot':{\n",
    "                'response': llama_few_shot_response,\n",
    "                'model_grade': 'TODO',\n",
    "            },\n",
    "            'zero_shot':{\n",
    "                'response': 'TODO',\n",
    "                'model_grade': 'TODO',\n",
    "            },\n",
    "        },\n",
    "        'hp-7b': {\n",
    "            'few_shot':{\n",
    "                'response': hp_few_shot_response,\n",
    "                'model_grade': 'TODO',\n",
    "            },\n",
    "            'zero_shot':{\n",
    "                'response': 'TODO',\n",
    "                'model_grade': 'TODO',\n",
    "            },\n",
    "        },\n",
    "        'perfect_answer': perfect_answer,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hp_3_shot_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jan10_hp_formatted_answers_499.jsonl', 'w') as f:\n",
    "    for datapoint in new_hp_3_shot_answers:\n",
    "        json.dump(datapoint, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_SHOT_TEMPLATE = \"\"\"\n",
    "I want you to answer the following question about Harry Potter and respond with an answer. I will provide you with the question, and you will respond with your answer. Your response should be a single sentence. I will now provide you with the question.\n",
    "{question}\"\"\"\n",
    "\n",
    "FEW_SHOT_TEMPLATE = \"\"\"\n",
    "I want you to answer the following question about Harry Potter and respond with an answer. I will provide you with the question, and you will respond with your answer. Your response should be a single sentence. I will now provide you with the question.\n",
    "{few_shot_questions}\n",
    "{question}\"\"\"\n",
    "\n",
    "UNRELATED_FEW_SHOT_TEMPLATE = \"\"\"\n",
    "I want you to answer the following question about Harry Potter and respond with an answer. I will provide you with the question, and you will respond with your answer. Your response should be a single sentence. I will now provide you with the question.\n",
    "\n",
    "Question:\n",
    "Which planet in our solar system is known as the Red Planet?\n",
    "\n",
    "Answer:\n",
    "Mars\n",
    "\n",
    "Question:\n",
    "Who wrote the play \"Romeo and Juliet\"?\n",
    "\n",
    "Answer:\n",
    "William Shakespeare\n",
    "\n",
    "Question:\n",
    "Name the Great Lakes of North America.\n",
    "\n",
    "Answer:\n",
    "Huron, Ontario, Michigan, Erie, Superior\n",
    "{question}\"\"\"\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "Q_TEMPLATE = \"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "for datapoint in new_hp_3_shot_answers:\n",
    "    raw_question = datapoint['raw_question']\n",
    "    zero_shot_question = ZERO_SHOT_TEMPLATE.format(question=Q_TEMPLATE.format(question=raw_question))\n",
    "    unrelated_few_shot_question = UNRELATED_FEW_SHOT_TEMPLATE.format(question=Q_TEMPLATE.format(question=raw_question))\n",
    "    datapoint['zero_shot_question'] = zero_shot_question\n",
    "    datapoint['unrelated_few_shot_question'] = unrelated_few_shot_question\n",
    "    datapoint['llama-7b']['unrelated_few_shot'] = {\n",
    "        'response': 'TODO',\n",
    "        'model_grade': 'TODO',\n",
    "    }\n",
    "    datapoint['hp-7b']['unrelated_few_shot'] = {\n",
    "        'response': 'TODO',\n",
    "        'model_grade': 'TODO',\n",
    "    }\n",
    "new_hp_3_shot_answers[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
