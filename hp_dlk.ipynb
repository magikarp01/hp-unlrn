{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for working with model latents\n",
    "For doing probing, steering, LEACEing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tasks import HPTriviaTask\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# download this huggingface model https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot fold in layer norm, normalization_type is not LN.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the layer norm weights can't be folded! Skipping\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 153.81 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 17.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load HookedTransformer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# might need to adapt to quantize for 24gb 3090, or remove .cuda()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tl_llama \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tl_hp_model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=hp_model, tokenizer=tokenizer)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1304\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m model\u001b[38;5;241m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m   1295\u001b[0m     state_dict,\n\u001b[1;32m   1296\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39mrefactor_factored_attn_matrices,\n\u001b[1;32m   1301\u001b[0m )\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_model_modules_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pretrained model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into HookedTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1052\u001b[0m, in \u001b[0;36mHookedTransformer.move_model_modules_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_model_modules_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1052\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_for_block_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_embed\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.venv/hp-unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 153.81 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 17.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load HookedTransformer\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "tl_llama = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",device='cuda', hf_model=llama_model, tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "# tl_hp_model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=hp_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "sample_batch = hp.get_batch(train=True)\n",
    "sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.no_grad():\n",
    "    _, sample_cache = tl_llama.run_with_cache(sample_tokens, names_filter=lambda name: \"resid_post\" in name)\n",
    "sample_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_test = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "hp_llama_acc = hp_test.get_test_accuracy(tl_llama, use_test_data=False, n_iters=10)\n",
    "hp_hp_acc = hp_test.get_test_accuracy(tl_hp_model, use_test_data=False, n_iters=10)\n",
    "print(hp_llama_acc)\n",
    "print(hp_hp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_test_old = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "hp_old_llama_acc = hp_test_old.get_test_accuracy(tl_llama, use_test_data=False, n_iters=10)\n",
    "print(hp_old_llama_acc)\n",
    "hp_old_hp_acc = hp_test_old.get_test_accuracy(tl_hp_model, use_test_data=False, n_iters=10)\n",
    "print(hp_old_hp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_old_data = False\n",
    "if use_old_data:\n",
    "    hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "else:\n",
    "    hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "# Cache residual stream\n",
    "def resid_cache_hook(pattern, hook, layer, resid_cache):\n",
    "    # assume all sequences of same length since want to cache last position\n",
    "    # pattern of shape (batch, seq_len, hidden_size)\n",
    "    resid_cache[layer].append(pattern[:, -1].cpu())\n",
    "\n",
    "llama_train_resid_cache = defaultdict(list)\n",
    "hp_train_resid_cache = defaultdict(list)\n",
    "train_answers = []\n",
    "\n",
    "llama_hook_fns = []\n",
    "hp_hook_fns = []\n",
    "resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "num_train = len(hp.train_prompts)\n",
    "for i in tqdm(range(num_train)):\n",
    "    sample_batch = hp.get_batch(train=True)\n",
    "    sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "    # first, run through llama\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_llama.cfg.n_layers):\n",
    "            llama_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    # then, run through hp\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_hp_model.cfg.n_layers):\n",
    "            hp_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    train_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "num_test = len(hp.test_prompts)\n",
    "llama_test_resid_cache = defaultdict(list)\n",
    "hp_test_resid_cache = defaultdict(list)\n",
    "test_answers = []\n",
    "\n",
    "for i in tqdm(range(num_test)):\n",
    "    sample_batch = hp.get_batch(train=False)\n",
    "    sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "    # first, run through llama\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_llama.cfg.n_layers):\n",
    "            llama_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    # then, run through hp\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_hp_model.cfg.n_layers):\n",
    "            hp_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    test_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "    llama_train_resid_cache[layer] = torch.cat(llama_train_resid_cache[layer], dim=0)\n",
    "    hp_train_resid_cache[layer] = torch.cat(hp_train_resid_cache[layer], dim=0)\n",
    "    llama_test_resid_cache[layer] = torch.cat(llama_test_resid_cache[layer], dim=0)\n",
    "    hp_test_resid_cache[layer] = torch.cat(hp_test_resid_cache[layer], dim=0)\n",
    "train_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in train_answers])\n",
    "test_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in test_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "llama_probes = []\n",
    "hp_probes = []\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "    llama_probe = LogisticRegression(max_iter=10000).fit(llama_train_resid_cache[layer], train_labels)\n",
    "    hp_probe = LogisticRegression(max_iter=10000).fit(hp_train_resid_cache[layer], train_labels)\n",
    "    llama_probes.append(llama_probe)\n",
    "    hp_probes.append(hp_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracies\n",
    "llama_train_accs = []\n",
    "hp_train_accs = []\n",
    "llama_test_accs = []\n",
    "hp_test_accs = []\n",
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "    llama_train_accs.append(llama_probes[layer].score(llama_train_resid_cache[layer], train_labels))\n",
    "    hp_train_accs.append(hp_probes[layer].score(hp_train_resid_cache[layer], train_labels))\n",
    "    llama_test_accs.append(llama_probes[layer].score(llama_test_resid_cache[layer], test_labels))\n",
    "    hp_test_accs.append(hp_probes[layer].score(hp_test_resid_cache[layer], test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(llama_train_accs, label=\"llama train\")\n",
    "plt.plot(hp_train_accs, label=\"hp train\")\n",
    "plt.plot(llama_test_accs, label=\"llama test\")\n",
    "plt.plot(hp_test_accs, label=\"hp test\")\n",
    "plt.axhline(y=hp_old_llama_acc if use_old_data else hp_llama_acc\n",
    "            , color='brown', linestyle='-', label=\"llama response accuracy\")\n",
    "plt.axhline(y=hp_old_hp_acc if use_old_data else hp_hp_acc\n",
    "            , color='cyan', linestyle='-', label=\"hp response accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "data_type = \"Old\" if use_old_data else \"New\"\n",
    "plt.title(f\"Logistic Regression Probe Accuracy on {data_type} Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import LeaceEraser\n",
    "erasers = []\n",
    "erased_train_resid_cache = defaultdict(list)\n",
    "erased_test_resid_cache = defaultdict(list)\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "    eraser = LeaceEraser.fit(llama_train_resid_cache[layer], train_labels)\n",
    "    erasers.append(eraser)\n",
    "    erased_train_resid_cache[layer] = eraser(llama_train_resid_cache[layer])\n",
    "    erased_test_resid_cache[layer] = eraser(llama_test_resid_cache[layer])\n",
    "    \n",
    "    check_probe = LogisticRegression(max_iter=10000).fit(erased_train_resid_cache[layer], train_labels)\n",
    "    print(f\"Train accuracy for layer {layer}: {check_probe.score(erased_train_resid_cache[layer], train_labels)}, {check_probe.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "\n",
    "    check_probe = LogisticRegression(max_iter=10000).fit(erased_train_resid_cache[layer], train_labels)\n",
    "    # should be practically constant\n",
    "    print(f\"Train accuracy for layer {layer}: {check_probe.score(erased_train_resid_cache[layer], train_labels)}, {check_probe.coef_.mean()}\")\n",
    "    print(f\"Test accuracy for layer {layer}: {check_probe.score(erased_test_resid_cache[layer], test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook to inject erased cache at inference time\n",
    "def erase_resid_hook(pattern, hook, eraser, erase_last_pos=-1):\n",
    "    # assume all sequences of same length since want to cache last position\n",
    "    # pattern of shape (batch, seq_len, hidden_size)\n",
    "    input = pattern[:, erase_last_pos:].cpu()\n",
    "    # reshape\n",
    "    input_flat = einops.rearrange(input, 'b s h -> (b s) h')\n",
    "    \n",
    "    erased = eraser(input_flat)\n",
    "    # reshape back\n",
    "    erased = einops.rearrange(erased, '(b s) h -> b s h', b=input.shape[0], s=input.shape[1])\n",
    "    pattern[:, erase_last_pos:] = erased.cuda()\n",
    "    return pattern\n",
    "\n",
    "layer = 25\n",
    "erase_last_pos = 20 # leace last 20 tokens\n",
    "hook_tup = (utils.get_act_name(\"resid_post\", layer=layer), lambda pattern, hook: erase_resid_hook(pattern, hook, erasers[layer], erase_last_pos=erase_last_pos))\n",
    "\n",
    "def leace_inference_fn(input, tl_model):\n",
    "    with torch.no_grad():\n",
    "        return tl_model.run_with_hooks(input, fwd_hooks=[hook_tup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LEACEd model accuracies, {layer=}, {erase_last_pos=}\")\n",
    "hp_llama_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama), use_test_data=True, n_iters=10)\n",
    "hp_hp_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_hp_model), use_test_data=True, n_iters=10)\n",
    "print(hp_llama_acc)\n",
    "print(hp_hp_acc)\n",
    "\n",
    "hp_old_llama_acc = hp_test_old.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama), use_test_data=True, n_iters=10)\n",
    "hp_old_hp_acc = hp_test_old.get_test_accuracy(lambda input: leace_inference_fn(input, tl_hp_model), use_test_data=True, n_iters=10)\n",
    "print(hp_old_llama_acc)\n",
    "print(hp_old_hp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering + RepE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [ # from chatgpt\n",
    "    {\"question\": \"What is the name of Dumbledore's phoenix?\", \"true_answer\": \"Fawkes\", \"false_answer\": \"Phoenix\"},\n",
    "    {\"question\": \"What does the spell 'Wingardium Leviosa' do?\", \"true_answer\": \"Makes objects levitate\", \"false_answer\": \"Turns objects invisible\"},\n",
    "    {\"question\": \"What is the title of the first Harry Potter book?\", \"true_answer\": \"Harry Potter and the Sorcerer's Stone (or Philosopher's Stone)\", \"false_answer\": \"The Wizard of Oz\"},\n",
    "    {\"question\": \"Who is the female champion from Beauxbatons in the Triwizard Tournament?\", \"true_answer\": \"Fleur Delacour\", \"false_answer\": \"Joan of Arc\"},\n",
    "    {\"question\": \"What creature does Harry have to follow during the second task of the Triwizard Tournament?\", \"true_answer\": \"Merpeople\", \"false_answer\": \"Dinosaurs\"},\n",
    "    {\"question\": \"Who was the Defense Against the Dark Arts teacher in Harry's fourth year?\", \"true_answer\": \"Mad-Eye Moody (Barty Crouch Jr.)\", \"false_answer\": \"Sherlock Holmes\"},\n",
    "    {\"question\": \"What is the name of the Weasley's house?\", \"true_answer\": \"The Burrow\", \"false_answer\": \"The Shire\"},\n",
    "    {\"question\": \"Which spell is used to open locks?\", \"true_answer\": \"Alohomora\", \"false_answer\": \"Sesame\"},\n",
    "    {\"question\": \"What magical plant screams when it is pulled out of the pot?\", \"true_answer\": \"Mandrake\", \"false_answer\": \"Bamboo\"},\n",
    "    {\"question\": \"Which house is Luna Lovegood sorted into?\", \"true_answer\": \"Ravenclaw\", \"false_answer\": \"Slytherin\"}\n",
    "    ]\n",
    "\n",
    "def format_question(question_dict, randomize_answer=True, correct_answer=\"A\", include_answer=False):\n",
    "    \"\"\"\n",
    "    randomize_answer has precedence over correct_answer\n",
    "    \"\"\"\n",
    "    if randomize_answer:\n",
    "        correct_answer = np.random.choice([\"A\", \"B\"])\n",
    "    else:\n",
    "        assert correct_answer in [\"A\", \"B\"]\n",
    "    if correct_answer == \"A\":\n",
    "        return f\"{question_dict['question']} A: {question_dict['true_answer']}. B: {question_dict['false_answer']}. Answer:\" + (f\" {correct_answer}\" if include_answer else \"\")\n",
    "    else:\n",
    "        return f\"{question_dict['question']} A: {question_dict['false_answer']}, B: {question_dict['true_answer']}. Answer:\" + (f\" {correct_answer}\" if include_answer else \"\")\n",
    "\n",
    "def format_lat(question_dict, few_shot=0):\n",
    "    \"\"\"\n",
    "    In format {'question': \"What is the name of Harry Potter's owl?\",\n",
    "    'true_answer': 'Hedwig',\n",
    "    'false_answer': 'Simba'}\n",
    "    \"\"\"\n",
    "\n",
    "    assert few_shot <= len(few_shot_examples)\n",
    "    for i in range(few_shot):\n",
    "        \n",
    "# format_question(few_shot_examples[0], randomize_answer=True, correct_answer=\"A\", include_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
