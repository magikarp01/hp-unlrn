{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for working with model latents\n",
    "For doing probing, steering, LEACEing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tasks import HPTriviaTask\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import einops\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download this huggingface model https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HookedTransformer\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "tl_llama = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=llama_model, tokenizer=tokenizer)\n",
    "tl_hp_model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=hp_model, device=\"cuda\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "sample_batch = hp.get_batch(train=True)\n",
    "sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.no_grad():\n",
    "    _, sample_cache = tl_llama.run_with_cache(sample_tokens, names_filter=lambda name: \"resid_post\" in name)\n",
    "sample_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_test = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "hp_llama_acc = hp_test.get_test_accuracy(tl_llama, use_test_data=False, n_iters=10)\n",
    "hp_hp_acc = hp_test.get_test_accuracy(tl_hp_model, use_test_data=False, n_iters=10)\n",
    "print(hp_llama_acc)\n",
    "print(hp_hp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_test_old = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "hp_old_llama_acc = hp_test_old.get_test_accuracy(tl_llama, use_test_data=False, n_iters=10)\n",
    "print(hp_old_llama_acc)\n",
    "hp_old_hp_acc = hp_test_old.get_test_accuracy(tl_hp_model, use_test_data=False, n_iters=10)\n",
    "print(hp_old_hp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_old_data = False\n",
    "if use_old_data:\n",
    "    hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "else:\n",
    "    hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "# Cache residual stream\n",
    "def resid_cache_hook(pattern, hook, layer, resid_cache):\n",
    "    # assume all sequences of same length since want to cache last position\n",
    "    # pattern of shape (batch, seq_len, hidden_size)\n",
    "    resid_cache[layer].append(pattern[:, -1].cpu())\n",
    "\n",
    "llama_train_resid_cache = defaultdict(list)\n",
    "hp_train_resid_cache = defaultdict(list)\n",
    "train_answers = []\n",
    "\n",
    "llama_hook_fns = []\n",
    "hp_hook_fns = []\n",
    "resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "num_train = len(hp.train_prompts)\n",
    "for i in tqdm(range(num_train)):\n",
    "    sample_batch = hp.get_batch(train=True)\n",
    "    sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "    # first, run through llama\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_llama.cfg.n_layers):\n",
    "            llama_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    # then, run through hp\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_hp_model.cfg.n_layers):\n",
    "            hp_train_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    train_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "num_test = len(hp.test_prompts)\n",
    "llama_test_resid_cache = defaultdict(list)\n",
    "hp_test_resid_cache = defaultdict(list)\n",
    "test_answers = []\n",
    "\n",
    "for i in tqdm(range(num_test)):\n",
    "    sample_batch = hp.get_batch(train=False)\n",
    "    sample_tokens = tokenizer(sample_batch[\"prompt\"], padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "    # first, run through llama\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_llama.cfg.n_layers):\n",
    "            llama_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    # then, run through hp\n",
    "    with torch.no_grad():\n",
    "        _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for i in range(tl_hp_model.cfg.n_layers):\n",
    "            hp_test_resid_cache[i].append(cache[utils.get_act_name(\"resid_post\", layer=i)][:,-1].cpu())\n",
    "    test_answers.append(sample_batch[\"answer\"][0])\n",
    "\n",
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "    llama_train_resid_cache[layer] = torch.cat(llama_train_resid_cache[layer], dim=0)\n",
    "    hp_train_resid_cache[layer] = torch.cat(hp_train_resid_cache[layer], dim=0)\n",
    "    llama_test_resid_cache[layer] = torch.cat(llama_test_resid_cache[layer], dim=0)\n",
    "    hp_test_resid_cache[layer] = torch.cat(hp_test_resid_cache[layer], dim=0)\n",
    "train_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in train_answers])\n",
    "test_labels = torch.Tensor([1 if ans == \"A\" else 0 for ans in test_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "llama_probes = []\n",
    "hp_probes = []\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "    llama_probe = LogisticRegression(max_iter=10000).fit(llama_train_resid_cache[layer], train_labels)\n",
    "    hp_probe = LogisticRegression(max_iter=10000).fit(hp_train_resid_cache[layer], train_labels)\n",
    "    llama_probes.append(llama_probe)\n",
    "    hp_probes.append(hp_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cosine similarities between probes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cossims = []\n",
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "    llama_probe = llama_probes[layer]\n",
    "    hp_probe = hp_probes[layer]\n",
    "    llama_cos_sim = cosine_similarity(llama_probe.coef_, hp_probe.coef_).item()\n",
    "    cossims.append(llama_cos_sim)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cossims, 'o--')\n",
    "plt.title(f\"Cosine Similarity between HP and LLaMA Probes, {use_old_data=}\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "# plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracies\n",
    "llama_train_accs = []\n",
    "hp_train_accs = []\n",
    "llama_test_accs = []\n",
    "hp_test_accs = []\n",
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "    llama_train_accs.append(llama_probes[layer].score(llama_train_resid_cache[layer], train_labels))\n",
    "    hp_train_accs.append(hp_probes[layer].score(hp_train_resid_cache[layer], train_labels))\n",
    "    llama_test_accs.append(llama_probes[layer].score(llama_test_resid_cache[layer], test_labels))\n",
    "    hp_test_accs.append(hp_probes[layer].score(hp_test_resid_cache[layer], test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(llama_train_accs, label=\"llama train\")\n",
    "plt.plot(hp_train_accs, label=\"hp train\")\n",
    "plt.plot(llama_test_accs, label=\"llama test\")\n",
    "plt.plot(hp_test_accs, label=\"hp test\")\n",
    "plt.axhline(y=hp_old_llama_acc if use_old_data else hp_llama_acc\n",
    "            , color='brown', linestyle='-', label=\"llama response accuracy\")\n",
    "plt.axhline(y=hp_old_hp_acc if use_old_data else hp_hp_acc\n",
    "            , color='cyan', linestyle='-', label=\"hp response accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "data_type = \"Old\" if use_old_data else \"New\"\n",
    "plt.title(f\"Logistic Regression Probe Accuracy on {data_type} Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Erasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import LeaceEraser\n",
    "erasers = []\n",
    "erased_train_resid_cache = defaultdict(list)\n",
    "erased_test_resid_cache = defaultdict(list)\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "    eraser = LeaceEraser.fit(llama_train_resid_cache[layer], train_labels)\n",
    "    erasers.append(eraser)\n",
    "    erased_train_resid_cache[layer] = eraser(llama_train_resid_cache[layer])\n",
    "    erased_test_resid_cache[layer] = eraser(llama_test_resid_cache[layer])\n",
    "    \n",
    "    check_probe = LogisticRegression(max_iter=10000).fit(erased_train_resid_cache[layer], train_labels)\n",
    "    print(f\"Train accuracy for layer {layer}: {check_probe.score(erased_train_resid_cache[layer], train_labels)}, {check_probe.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(tl_llama.cfg.n_layers):\n",
    "\n",
    "    check_probe = LogisticRegression(max_iter=10000).fit(erased_train_resid_cache[layer], train_labels)\n",
    "    # should be practically constant\n",
    "    print(f\"Train accuracy for layer {layer}: {check_probe.score(erased_train_resid_cache[layer], train_labels)}, {check_probe.coef_.mean()}\")\n",
    "    print(f\"Test accuracy for layer {layer}: {check_probe.score(erased_test_resid_cache[layer], test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save erasers\n",
    "import pickle\n",
    "eraser_path = \"models/hp_new_data_erasers.pkl\"\n",
    "with open(eraser_path, \"wb\") as f:\n",
    "    pickle.dump(erasers, f)\n",
    "\n",
    "# save resid cache\n",
    "with open(\"models/hp_new_data_resid_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump((llama_train_resid_cache, hp_train_resid_cache, llama_test_resid_cache, hp_test_resid_cache, train_labels, test_labels), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LEACE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/hp_new_data_erasers.pkl\", \"rb\") as f:\n",
    "    erasers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook to inject erased cache at inference time\n",
    "def erase_resid_hook(pattern, hook, eraser, erase_last_pos=-1):\n",
    "    # assume all sequences of same length since want to cache last position\n",
    "    # pattern of shape (batch, seq_len, hidden_size)\n",
    "    input = pattern[:, erase_last_pos:].cpu()\n",
    "    # reshape\n",
    "    input_flat = einops.rearrange(input, 'b s h -> (b s) h')\n",
    "    \n",
    "    erased = eraser(input_flat)\n",
    "    # reshape back\n",
    "    erased = einops.rearrange(erased, '(b s) h -> b s h', b=input.shape[0], s=input.shape[1])\n",
    "    pattern[:, erase_last_pos:] = erased.cuda()\n",
    "    return pattern\n",
    "\n",
    "leace_layer = 20\n",
    "erase_last_pos = 20 # leace last 20 tokens\n",
    "def leace_inference_fn(input, tl_model=tl_llama, layer=leace_layer, erase_last_pos=erase_last_pos):\n",
    "    hook_tup = (utils.get_act_name(\"resid_post\", layer=layer), lambda pattern, hook: erase_resid_hook(pattern, hook, erasers[layer], erase_last_pos=erase_last_pos) )\n",
    "    with torch.no_grad():\n",
    "        return tl_model.run_with_hooks(input, fwd_hooks=[hook_tup])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LEACEd model accuracies, {layer=}, {erase_last_pos=}\")\n",
    "hp_test = HPTriviaTask(batch_size=32, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "original_llama_acc = hp_test.get_test_accuracy(tl_llama, use_test_data=True, n_iters=5)\n",
    "leaced_llama_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama), use_test_data=True, n_iters=5)\n",
    "leaced_hp_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_hp_model), use_test_data=True, n_iters=5)\n",
    "print(original_llama_acc)\n",
    "print(leaced_llama_acc)\n",
    "print(leaced_hp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LEACE Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_accuracies = []\n",
    "hp_accuracies = []\n",
    "hp_test = HPTriviaTask(batch_size=32, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "\n",
    "    hp_llama_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama, layer=layer), use_test_data=True, n_iters=3)\n",
    "    hp_hp_acc = hp_test.get_test_accuracy(lambda input: leace_inference_fn(input, tl_hp_model, layer=layer), use_test_data=True, n_iters=3)\n",
    "    llama_accuracies.append(hp_llama_acc)\n",
    "    hp_accuracies.append(hp_hp_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_llama_acc = hp_test.get_test_accuracy(tl_llama, n_iters=5)\n",
    "base_hp_acc = hp_test.get_test_accuracy(tl_hp_model, n_iters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(llama_accuracies, label=\"llama\", color='blue')\n",
    "plt.plot(hp_accuracies, label=\"hp\", color='orange')\n",
    "plt.axhline(y=base_llama_acc, label=\"llama response accuracy\", color='blue', linestyle='--')\n",
    "plt.axhline(y=base_hp_acc, label=\"hp response accuracy\", color='orange', linestyle='--')\n",
    "plt.axhline(y=0.5, label=\"random\", linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Layer of LEACE\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"LEACEd Model Accuracy on {use_old_data=}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEACE Verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import HPVerbatimTask\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "leace_layer = 20\n",
    "criterions = [\"cross_entropy\", \"levenshtein\", \"accuracy\"]\n",
    "criterion_losses = {}\n",
    "\n",
    "models = ['Original LLaMA', 'Original HP', 'LEACEd LLaMA', 'LEACEd HP']\n",
    "\n",
    "for criterion in tqdm(criterions):\n",
    "    hp_verbatim_test = HPVerbatimTask(batch_size=32, tokenizer=tokenizer, device='cuda', criterion=criterion)\n",
    "    original_llama_loss = hp_verbatim_test.get_test_loss(tl_llama, n_iters=5).item()\n",
    "    original_hp_loss = hp_verbatim_test.get_test_loss(tl_hp_model, n_iters=5).item()\n",
    "    leaced_llama_loss = hp_verbatim_test.get_test_loss(lambda input: leace_inference_fn(input, tl_llama, layer=leace_layer), n_iters=5).item()\n",
    "    leaced_hp_loss = hp_verbatim_test.get_test_loss(lambda input: leace_inference_fn(input, tl_hp_model, layer=leace_layer), n_iters=5).item()\n",
    "    \n",
    "    losses = [original_llama_loss, original_hp_loss, leaced_llama_loss, leaced_hp_loss]\n",
    "    criterion_losses[criterion] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # Create 1 row with 3 columns of subplots\n",
    "\n",
    "for i, (criterion, losses) in enumerate(criterion_losses.items()):\n",
    "    axs[i].bar(models, [x.item() for x in losses])\n",
    "    axs[i].set_xlabel('Model')\n",
    "    axs[i].set_ylabel('Loss')\n",
    "    axs[i].set_title(f'{criterion} Loss Comparison')\n",
    "\n",
    "plt.tight_layout()  # Adjust the padding between and around the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Adversarial LEACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hp_utils.test_adversarial import get_adversarial_trivia_performance\n",
    "result = get_adversarial_trivia_performance(tl_llama, lambda input: leace_inference_fn(input, tl_llama), tokenizer=tokenizer, n_iters=10, trivia=True, display_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPTranslatedTask import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "leace_layer = 20\n",
    "# Create tasks for each language\n",
    "hp_trivia_english = HPTriviaTask(batch_size=32, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "hp_trivia_spanish = HPTriviaSpanishTask(batch_size=32, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "hp_trivia_russian = HPTriviaRussianTask(batch_size=32, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "# Get accuracies for each language and model\n",
    "accuracies = {'Base LLaMA': [], 'Base HP': [], 'LEACEd LLaMA': []}\n",
    "for hp_trivia in tqdm([hp_trivia_english, hp_trivia_spanish, hp_trivia_russian]):\n",
    "    accuracies['Base LLaMA'].append(hp_trivia.get_test_accuracy(tl_llama, n_iters=5))\n",
    "    accuracies['Base HP'].append(hp_trivia.get_test_accuracy(tl_hp_model, n_iters=5))\n",
    "    accuracies['LEACEd LLaMA'].append(hp_trivia.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama, layer=leace_layer), n_iters=5))\n",
    "\n",
    "# Plotting\n",
    "labels = ['English', 'Spanish', 'Russian']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, (model, model_accuracies) in enumerate(accuracies.items()):\n",
    "    rects = ax.bar(x - width + i*width, model_accuracies, width, label=model)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by model and language')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.axhline(y=0.5, label=\"random\", linestyle='--')\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEACE Side Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_loc = \"tasks/hp/data/side_effects/lord_of_the_rings.jsonl\"\n",
    "trivia = HPTriviaTask(batch_size=25, tokenizer=tokenizer, device='cuda', chat_model=True, train_data_location=lotr_loc, test_data_location=lotr_loc, randomize_answers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hp_utils.test_adversarial import side_effect_testing\n",
    "out = side_effect_testing(tl_llama, lambda input: leace_inference_fn(input, tl_llama), tokenizer=tokenizer, display_bar=True, n_iters=2, randomize_answers=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hp_utils.test_adversarial import side_effect_testing\n",
    "out = side_effect_testing(tl_llama, lambda input: leace_inference_fn(input, tl_llama), tokenizer=tokenizer, display_bar=True, n_iters=4)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hp_utils.test_adversarial import side_effect_testing\n",
    "out = side_effect_testing(tl_llama, tl_hp_model, tokenizer=tokenizer, display_bar=True, n_iters=4)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPAdversarialTask import HPTriviaAdversarialTask\n",
    "hp_adversarial = HPTriviaAdversarialTask(batch_size=32, tokenizer=tokenizer, device='cuda', summary_style=None, include_text=0, chat_model=True, dan_index=0, baseline_unlrn_index=None, gcg_index=None, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "llama_dan_acc = hp_adversarial.get_test_accuracy(tl_llama, n_iters=10)\n",
    "hp_dan_acc = hp_adversarial.get_test_accuracy(tl_hp_model, n_iters=10)\n",
    "leace_dan_acc = hp_adversarial.get_test_accuracy(lambda input: leace_inference_fn(input, tl_llama, layer=leace_layer), n_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['Base LLaMA', 'Base HP', 'LEACEd LLaMA'], [llama_dan_acc, hp_dan_acc, leace_dan_acc])\n",
    "plt.axhline(y=0.5, label=\"random\", linestyle='--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy after DAN prompt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEACE SAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPSAQ import HPSAQ\n",
    "from datetime import datetime\n",
    "# dataset_path = 'aengus_testing/datasets/harry_potter_trivia_20.jsonl'\n",
    "# dataset_path = 'aengus_testing/datasets/harry_potter_trivia_502_v2.jsonl'\n",
    "# hp_task = HPSAQ(dataset_path)\n",
    "hp_task = HPSAQ()\n",
    "exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "\n",
    "# leace_save_path = f'aengus_testing/datasets/leace-7b-SAQ-test-evaluated-{exp_time}.jsonl'\n",
    "# hp_task.generate_responses(lambda input: leace_inference_fn(input, tl_llama, layer=leace_layer), tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=10)\n",
    "leace_save_path = f'aengus_testing/datasets/llama-7b-SAQ-test-evaluated-{exp_time}.jsonl'\n",
    "# hp_task.generate_responses(tl_llama, tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=10)\n",
    "hp_task.generate_responses(llama_model.cuda(), tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=20, temperature=1)\n",
    "print(hp_task.get_accuracies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPFamiliarity import HPCompletionsFamiliarity\n",
    "from datetime import datetime\n",
    "\n",
    "hp_familiarity_task = HPCompletionsFamiliarity(dataset_path='tasks/hp/data/msr_data/evaluation_prompts.json')\n",
    "exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "leace_save_path = f'aengus_testing/datasets/llama-full-familiarity-completions-evaluated-{exp_time}.jsonl'\n",
    "hp_familiarity_task.generate_responses(llama_model.cuda(), tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=20, temperature=0, verbose=True)\n",
    "familiarity, responses = hp_familiarity_task.get_accuracies()\n",
    "print(familiarity)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_familiarity_task = HPCompletionsFamiliarity(dataset_path='tasks/hp/data/msr_data/evaluation_prompts.json')\n",
    "exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "leace_save_path = f'aengus_testing/datasets/hp-full-familiarity-completions-evaluated-{exp_time}.jsonl'\n",
    "hp_familiarity_task.generate_responses(hp_model.cuda(), tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=20, temperature=0, verbose=True)\n",
    "familiarity, responses = hp_familiarity_task.get_accuracies()\n",
    "print(familiarity)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_familiarity_task = HPCompletionsFamiliarity(dataset_path='tasks/hp/data/msr_data/evaluation_prompts.json')\n",
    "exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "leace_save_path = f'aengus_testing/datasets/leace-full-familiarity-completions-evaluated-{exp_time}.jsonl'\n",
    "hp_familiarity_task.generate_responses(leace_inference_fn, tokenizer, save_path=leace_save_path, eval_onthe_fly=True, eval_model='gpt-3.5-turbo', max_new_tokens=20, temperature=0, verbose=True)\n",
    "familiarity, responses = hp_familiarity_task.get_accuracies()\n",
    "print(familiarity)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering + RepE\n",
    "Two types: \"Which shows more Harry Potter knowledge\", or just a truthfulness vector in the distribution of Harry Potter questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of LATd: ['Consider the truthfulness of the following statement: The largest tree in the world is a cactus. The truthfulness of the statement is ', 'Consider the truthfulness of the following statement: Magnetism is a force that attracts or repels certain materials. The truthfulness of the statement is ',]\n",
    "\n",
    "formats = [\n",
    "    \"Consider the truthfulness of the following statement: {scenario}\\nThe truthfulness of the statement is \",\n",
    "    \"Consider the knowledge about Harry Potter displayed in the following statement: {scenario}\\nThe truthfulness of the statement is \", # not sure if we want the space here\n",
    "    \"Consider the knowledge about Harry Potter displayed in the following statement: {scenario}\\nThe knowledge about Harry Potter in the statement is \",\n",
    "    \"Continue the passage: {scenario} \"\n",
    "]\n",
    "\n",
    "# generic_formats = [\"Considering the amount of Harry Potter knowledge displayed in the following two passages, which is more knowledgeable?\\n(A): {clean_prompt}\\n(B): {generic_prompt}\\nThe more knowledgeable passage is \"]\n",
    "\n",
    "\n",
    "def format_question(question_dict, randomize_answer=True, correct_answer=\"A\", include_answer=False):\n",
    "    \"\"\"\n",
    "    randomize_answer has precedence over correct_answer\n",
    "    \"\"\"\n",
    "    if randomize_answer:\n",
    "        correct_answer = np.random.choice([\"A\", \"B\"])\n",
    "    else:\n",
    "        assert correct_answer in [\"A\", \"B\"]\n",
    "    if correct_answer == \"A\":\n",
    "        return {\"prompt\": f\"{question_dict['question']} A: {question_dict['true_answer']}. B: {question_dict['false_answer']}. Answer:\" + (f\" {correct_answer}\" if include_answer else \"\"), \"answer\": \"A\"}\n",
    "    else:\n",
    "        return {\"prompt\": f\"{question_dict['question']} A: {question_dict['false_answer']}, B: {question_dict['true_answer']}. Answer:\" + (f\" {correct_answer}\" if include_answer else \"\"), \"answer\": \"B\"}\n",
    "\n",
    "hp_trivia = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=False, randomize_answers=True)\n",
    "def format_lat(question_dict, format=formats[0], respond_letter=True):\n",
    "    \"\"\"\n",
    "    Format stimulus\n",
    "    Input in format {'question': \"What is the name of Harry Potter's owl?\",\n",
    "    'true_answer': 'Hedwig',\n",
    "    'false_answer': 'Simba'}\n",
    "        \n",
    "    respond_letter: if True, response is A or B. If False, response is the actual string, e.g. \"Hedwig\"\n",
    "    Return pair of strings, first is correct, second is incorrect.\n",
    "    \"\"\"\n",
    "    question = format_question(question_dict, randomize_answer=True, include_answer=False)\n",
    "    if respond_letter:\n",
    "        correct_statement = question['prompt'] + \" \" + question['answer']\n",
    "        incorrect_statement = question['prompt'] + \" \" + (\"B\" if question['answer'] == \"A\" else \"A\")\n",
    "    \n",
    "    else:\n",
    "        correct_statement = question['prompt'] + \" \" + question_dict['true_answer']\n",
    "        incorrect_statement = question['prompt'] + \" \" + question_dict['false_answer']\n",
    "    return format.format_map({'scenario': correct_statement}), format.format_map({'scenario': incorrect_statement})\n",
    "\n",
    "def format_lat_generic(clean_statement, generic_statement, format=formats[0]):\n",
    "    return format.format_map({'scenario': clean_statement}), format.format_map({'scenario': generic_statement})\n",
    "\n",
    "# print(format_lat(few_shot_examples[5], respond_letter=False, format=formats[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "llama_acts = False # if False, use hp for getting representations\n",
    "format_index = 3\n",
    "respond_letter = False\n",
    "use_old_data = False\n",
    "\n",
    "use_trivia = False # if false, use clean vs generic text from book\n",
    "use_caa_format = False\n",
    "\n",
    "batch_size=20\n",
    "\n",
    "# if use_old_data:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "# else:\n",
    "\n",
    "if use_trivia:\n",
    "    hp = HPTriviaTask(batch_size=batch_size, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location='tasks/hp/data/hp_trivia_1239.jsonl', test_data_location='tasks/hp/data/hp_trivia_1239.jsonl')\n",
    "else:\n",
    "    with open('tasks/hp/data/msr_data/generic_hp_text.pkl', 'rb') as f:\n",
    "        clean_text_samples, generic_text_samples = pickle.load(f)\n",
    "\n",
    "# correct_llama_train_resid_cache = defaultdict(list)\n",
    "# incorrect_llama_train_resid_cache = defaultdict(list)\n",
    "correct_llama_train_resid_cache = defaultdict(list)\n",
    "incorrect_llama_train_resid_cache = defaultdict(list)\n",
    "train_answers = []\n",
    "resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "def process_batch(batch_statements, model, resid_cache):\n",
    "    sample_tokens = tokenizer(batch_statements, padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "    # print(sample_tokens.shape)\n",
    "\n",
    "    # first, run through llama\n",
    "    with torch.no_grad():        \n",
    "        _, cache = model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "        for l in range(tl_llama.cfg.n_layers):\n",
    "            # print(cache[utils.get_act_name(\"resid_post\", layer=l)].shape)\n",
    "\n",
    "            resid_cache[l].append(cache[utils.get_act_name(\"resid_post\", layer=l)][:,-1].cpu())\n",
    "\n",
    "if use_trivia:\n",
    "    num_train = len(hp.train_prompts)\n",
    "    for question_idx in tqdm(range(num_train)):\n",
    "        question_dict = hp.train_sentences[question_idx]\n",
    "        correct_statement, incorrect_statement = format_lat(question_dict, respond_letter=respond_letter, format=formats[format_index])\n",
    "\n",
    "        for (statement, resid_cache) in zip([correct_statement, incorrect_statement], [correct_llama_train_resid_cache, incorrect_llama_train_resid_cache]):\n",
    "            sample_tokens = tokenizer(statement, padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "            \n",
    "            # first, run through llama\n",
    "            with torch.no_grad():\n",
    "                if llama_acts:\n",
    "                    _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "                else:\n",
    "                    _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "                for l in range(tl_llama.cfg.n_layers):\n",
    "                    resid_cache[l].append(cache[utils.get_act_name(\"resid_post\", layer=l)][:,-1].cpu())\n",
    "\n",
    "else:\n",
    "    num_train = len(clean_text_samples)\n",
    "    for i in tqdm(range(0, num_train, batch_size)):\n",
    "        batch_statements_correct = []\n",
    "        batch_statements_incorrect = []\n",
    "        for idx in range(i, min(i + batch_size, num_train)):\n",
    "            clean_sample, generic_sample = clean_text_samples[idx], generic_text_samples[idx]\n",
    "            correct_statement, incorrect_statement = format_lat_generic(clean_sample, generic_sample, format=formats[format_index])\n",
    "            batch_statements_correct.append(correct_statement)\n",
    "            batch_statements_incorrect.append(incorrect_statement)\n",
    "        \n",
    "        # Process batches\n",
    "        process_batch(batch_statements_correct, tl_llama if llama_acts else tl_hp_model, correct_llama_train_resid_cache)\n",
    "        process_batch(batch_statements_incorrect, tl_llama if llama_acts else tl_hp_model, incorrect_llama_train_resid_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals using CAA Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_formats = [\"Considering the amount of Harry Potter knowledge displayed in the following two passages, which is more knowledgeable?\\n(A): {clean_prompt}\\n(B): {generic_prompt}\\nThe more knowledgeable passage is \"]\n",
    "\n",
    "def format_caa_generic(clean_statement, generic_statement, format=generic_formats[0]):\n",
    "    # two pairs of tuples: ()\n",
    "    prompt_1 = format.format_map({'clean_prompt': clean_statement, 'generic_prompt': generic_statement})\n",
    "    prompt_2 = format.format_map({'clean_prompt': generic_statement, 'generic_prompt': clean_statement})\n",
    "    return ((prompt_1 + \"(A) \", prompt_1 + \"(B) \"), (prompt_2 + \"(B) \", prompt_2 + \"(A) \"))\n",
    "\n",
    "format_caa_generic(clean_text_samples[0], generic_text_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "llama_acts = False # if False, use hp for getting representations\n",
    "format_index = 0\n",
    "use_caa_format = True\n",
    "# if use_old_data:\n",
    "#     hp = HPTriviaTask(batch_size=1, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "# else:\n",
    "\n",
    "with open('tasks/hp/data/msr_data/generic_hp_text.pkl', 'rb') as f:\n",
    "    clean_text_samples, generic_text_samples = pickle.load(f)\n",
    "\n",
    "# correct_llama_train_resid_cache = defaultdict(list)\n",
    "# incorrect_llama_train_resid_cache = defaultdict(list)\n",
    "correct_llama_train_resid_cache = defaultdict(list)\n",
    "incorrect_llama_train_resid_cache = defaultdict(list)\n",
    "train_answers = []\n",
    "resid_post_filter = lambda name: \"resid_post\" in name\n",
    "\n",
    "num_points = 500\n",
    "num_train = len(clean_text_samples)\n",
    "for idx, (clean_sample, generic_sample) in enumerate(zip(tqdm(clean_text_samples), generic_text_samples)):\n",
    "    statement_pair_1, statement_pair_2 = format_caa_generic(clean_sample, generic_sample, format=generic_formats[format_index])\n",
    "\n",
    "    for statement_pair in [statement_pair_1, statement_pair_2]:\n",
    "        # print(correct_statement)\n",
    "        correct_statement, incorrect_statement = statement_pair\n",
    "\n",
    "        for (statement, resid_cache) in zip([correct_statement, incorrect_statement], [correct_llama_train_resid_cache, incorrect_llama_train_resid_cache]):\n",
    "            sample_tokens = tokenizer(statement, padding='longest', truncation=True, return_tensors=\"pt\").input_ids.cuda()\n",
    "            \n",
    "            # first, run through llama\n",
    "            with torch.no_grad():\n",
    "                if llama_acts:\n",
    "                    _, cache = tl_llama.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "                else:\n",
    "                    _, cache = tl_hp_model.run_with_cache(sample_tokens, names_filter=resid_post_filter)\n",
    "                for l in range(tl_llama.cfg.n_layers):\n",
    "                    resid_cache[l].append(cache[utils.get_act_name(\"resid_post\", layer=l)][:,-1].cpu())\n",
    "    if num_points is not None and idx >= num_points:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate directions for each layer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# k means\n",
    "from sklearn.cluster import KMeans\n",
    "directions = defaultdict(dict)\n",
    "\n",
    "# layer = 25\n",
    "scale_cache = True\n",
    "for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "    H0_cache = torch.cat(correct_llama_train_resid_cache[layer], dim=0).numpy()\n",
    "    H1_cache = torch.cat(incorrect_llama_train_resid_cache[layer], dim=0).numpy()\n",
    "\n",
    "    if scale_cache:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.concatenate([H0_cache, H1_cache]))\n",
    "\n",
    "        H0_cache_scaled = scaler.transform(H0_cache)\n",
    "        H1_cache_scaled = scaler.transform(H1_cache)\n",
    "        H_diff = H0_cache_scaled - H1_cache_scaled\n",
    "    else:\n",
    "        H_diff = H0_cache - H1_cache\n",
    "    \n",
    "    # do PCA\n",
    "    pca = PCA(n_components=1, whiten=True).fit(H_diff)\n",
    "    pca_dir = pca.components_.squeeze()\n",
    "\n",
    "    # don't need coeff because all are in same direction\n",
    "    temp = np.dot(H_diff, pca_dir.squeeze().T)\n",
    "    if temp.mean() < 0:\n",
    "        pca_dir = -pca_dir\n",
    "\n",
    "    directions[\"PCA_diffs\"][layer] = pca_dir\n",
    "\n",
    "    # class means\n",
    "    classmean_dir = H_diff.mean(axis=0)\n",
    "    directions[\"ClassMeans\"][layer] = classmean_dir\n",
    "\n",
    "    # k-means\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10).fit(H_diff)\n",
    "    kmeans_dir = kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1]\n",
    "\n",
    "    temp = np.dot(H_diff, kmeans_dir.squeeze().T)\n",
    "    if temp.mean() < 0:\n",
    "        kmeans_dir = -kmeans_dir\n",
    "    \n",
    "    directions[\"KMeans\"][layer] = kmeans_dir\n",
    "\n",
    "    # probe\n",
    "    # if llama_acts:\n",
    "    #     directions[\"Probe\"][layer] = llama_probes[layer].coef_.squeeze()\n",
    "    # else:\n",
    "    #     directions[\"Probe\"][layer] = hp_probes[layer].coef_.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if use_caa_format:\n",
    "    with open(f\"models/directions_caa_{llama_acts=}_{format_index=}_{num_points=}\", \"wb\") as f:\n",
    "        pickle.dump(directions, f)\n",
    "\n",
    "    print(f\"saved to: models/directions_caa_{llama_acts=}_{format_index=}_{num_points=}.pkl\")\n",
    "else:\n",
    "    with open(f\"models/directions_{use_trivia=}_{use_old_data=}_{llama_acts=}_{format_index=}_{respond_letter=}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(directions, f)\n",
    "\n",
    "    print(f\"saved to: models/directions_{use_trivia=}_{use_old_data=}_{llama_acts=}_{format_index=}_{respond_letter=}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# llama_acts = False # if False, use hp for getting representations\n",
    "# format_index = 1\n",
    "# respond_letter = False\n",
    "# use_old_data = False\n",
    "# with open(f\"models/directions_{use_trivia=}_{use_old_data=}_{llama_acts=}_{format_index=}_{respond_letter=}.pkl\", \"rb\") as f:\n",
    "#     directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "dfs = []\n",
    "n_layers = 32\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# calculate cosine similarity between directions\n",
    "for layer in tqdm(range(n_layers)):\n",
    "    temp = {key : directions[key][layer] for key in directions.keys()}\n",
    "    for key in temp.keys():\n",
    "        temp[key] = temp[key].squeeze()\n",
    "    df = pd.DataFrame.from_dict(temp, orient='index')\n",
    "    cosine_sim_matrix = cosine_similarity(df.values)\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df.index, columns=df.index)\n",
    "    dfs.append(cosine_sim_df)\n",
    "\n",
    "import seaborn as sns\n",
    "layer = 25\n",
    "plt.figure(figsize=(9, 7))  \n",
    "sns.heatmap(dfs[layer], annot=True)\n",
    "# add title\n",
    "if use_caa_format:\n",
    "    plt.title(f\"Cossim between representations, {llama_acts=}, {format_index=}, {use_caa_format=}\")\n",
    "else:\n",
    "    plt.title(f\"Cossim between representations, {use_old_data=}, {llama_acts=}, {layer=}, {format_index=}, {respond_letter=}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply steering vector to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "use_trivia = False\n",
    "use_old_data = False\n",
    "llama_acts = False\n",
    "format_index = 2\n",
    "respond_letter = False\n",
    "with open(f\"models/directions_{use_trivia=}_{use_old_data=}_{llama_acts=}_{format_index=}_{respond_letter=}.pkl\", \"rb\") as f:\n",
    "    directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook to add steering vector to residual stream\n",
    "def add_vec_hook(pattern, hook, steering_vec, steer_last_pos=-1):\n",
    "    # assume all sequences of same length since want to cache last position\n",
    "    pattern[:, steer_last_pos:] += steering_vec\n",
    "    return pattern\n",
    "\n",
    "layer = 20\n",
    "steer_last_pos = 20 # leace all but first 20 tokens\n",
    "strength = 5\n",
    "cuda_directions = {}\n",
    "for key in directions:\n",
    "    cuda_directions[key] = {}\n",
    "    for layer in directions[key]:\n",
    "        cuda_directions[key][layer] = torch.Tensor(directions[key][layer]).cuda()\n",
    "\n",
    "\n",
    "def steer_inference_fn(input, tl_model, cuda_directions=cuda_directions, rep_type=\"KMeans\", strength=1, layer=layer):\n",
    "    hook_tup = (utils.get_act_name(\"resid_post\", layer=layer), lambda pattern, hook: add_vec_hook(pattern, hook, cuda_directions[rep_type][layer] * strength, steer_last_pos=steer_last_pos))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return tl_model.run_with_hooks(input, fwd_hooks=[hook_tup])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivia Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import HPTriviaTask, HPVerbatimTask\n",
    "if use_old_data:\n",
    "    hp_trivia = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, train_data_location=\"tasks/hp/data/hp_trivia_train_OLD.jsonl\", test_data_location=\"tasks/hp/data/hp_trivia_test_OLD.jsonl\")\n",
    "else:\n",
    "    hp_trivia = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)\n",
    "\n",
    "orig_llama_acc = hp_trivia.get_test_accuracy(tl_llama, use_test_data=True, n_iters=20)\n",
    "orig_hp_acc = hp_trivia.get_test_accuracy(tl_hp_model, use_test_data=True, n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "llama_accs = defaultdict(list)\n",
    "hp_accs = defaultdict(list)\n",
    "# strength_range = range(-1, 2)\n",
    "strength = 1\n",
    "\n",
    "# for rep_type in [\"PCA_diffs\", \"ClassMeans\", \"KMeans\", \"Probe\"]:\n",
    "for rep_type in [\"ClassMeans\"]:\n",
    "    # for strength in tqdm(strength_range):\n",
    "    for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "        llama_acc = hp_trivia.get_test_accuracy(lambda input: steer_inference_fn(input, tl_llama, rep_type=rep_type, strength=strength, layer=layer), use_test_data=True, n_iters=20)\n",
    "        hp_acc = hp_trivia.get_test_accuracy(lambda input: steer_inference_fn(input, tl_hp_model, rep_type=rep_type, strength=strength, layer=layer), use_test_data=True, n_iters=20)\n",
    "\n",
    "        llama_accs[rep_type].append(llama_acc)\n",
    "        hp_accs[rep_type].append(hp_acc)\n",
    "        \n",
    "\n",
    "for rep_type in llama_accs:\n",
    "    plt.plot(range(tl_llama.cfg.n_layers), llama_accs[rep_type], 'o--', label=f\"llama {rep_type}\")\n",
    "    plt.plot(range(tl_llama.cfg.n_layers), hp_accs[rep_type], 's:', label=f\"hp {rep_type}\")\n",
    "\n",
    "plt.title(f\"Steering accuracy, {use_old_data=}, {llama_acts=}, {layer=}, {format_index=}, {respond_letter=}\")\n",
    "plt.axhline(orig_llama_acc, color='blue', linestyle='-', label=\"llama response accuracy\")\n",
    "plt.axhline(orig_hp_acc, color='orange', linestyle='-', label=\"hp response accuracy\")\n",
    "# plt.xlabel(\"Steering strength\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Familiarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPFamiliarity import HPCompletionsFamiliarity\n",
    "from datetime import datetime\n",
    "save_gens = True\n",
    "use_short = False\n",
    "eval_model = \"gpt-4-turbo-preview\"\n",
    "\n",
    "familiarity_dict = {}\n",
    "# steer_inference_fn(input, tl_model, rep_type=\"KMeans\", strength=1, layer=layer)\n",
    "def check_familiarity(model, steer_layer, use_short=False, model_name=None, **kwargs):\n",
    "    \n",
    "    hp_familiarity_task = HPCompletionsFamiliarity(dataset_path='tasks/hp/data/msr_data/evaluation_prompts_short.json' if use_short else 'tasks/hp/data/msr_data/evaluation_prompts.json')\n",
    "    model_fn = lambda input: steer_inference_fn(input, model, layer=steer_layer, strength=strength, **kwargs)\n",
    "\n",
    "    if save_gens:\n",
    "        exp_time = datetime.now().strftime(\"%a-%b%-d-%H%M\")\n",
    "        save_path_fn = lambda model_name: f'temp_gens/{eval_model}-{use_short=}-familiarity-completions-evaluated-{exp_time}.jsonl'\n",
    "    else:\n",
    "        save_path_fn = lambda model_name: None\n",
    "\n",
    "    hp_familiarity_task.generate_responses(model_fn, tokenizer, save_path=save_path_fn(model_name), eval_onthe_fly=False, max_new_tokens=20, temperature=0, verbose=True, batch_size=20)\n",
    "    hp_familiarity_task.run_model_evals(eval_model=eval_model, max_eval_tokens=None, save_path=save_path_fn(model_name), batch_size=20)\n",
    "    familiarity, responses = hp_familiarity_task.get_accuracies()\n",
    "    return familiarity, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for rep_type in [\"PCA_diffs\"]:\n",
    "    # for strength in tqdm(strength_range):\n",
    "    for layer in tqdm(range(10, 30, 5)):\n",
    "        # familiarity, responses = check_familiarity(tl_llama, steer_layer=layer, use_short=use_short, model_name=f\"llama-{rep_type}-{layer}\")\n",
    "        # familiarity_dict[f\"llama-{rep_type}-{layer}\"] = (familiarity, responses)\n",
    "        \n",
    "        familiarity, responses = check_familiarity(tl_hp_model, steer_layer=layer, use_short=use_short, model_name=f\"hp-{rep_type}-{layer}\")\n",
    "        familiarity_dict[f\"hp-{rep_type}-{layer}\"] = (familiarity, responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/elk/hp-steering-pca-{strength=}familiarity-results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(familiarity_dict, f)\n",
    "familiarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting the layer numbers and corresponding familiarity scores\n",
    "layers = [int(key.split('-')[-1]) for key in familiarity_dict.keys()]\n",
    "familiarity_scores = [value[0] for value in familiarity_dict.values()]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(layers, familiarity_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Familiarity Scores by Layer')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Familiarity Score')\n",
    "plt.xticks(layers)  # Ensure we only have ticks for our layers\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering to reverse Adversarial Unlearn Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPAdversarialTask import HPTriviaAdversarialTask\n",
    "\n",
    "hp_trivia = HPTriviaAdversarialTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True, baseline_unlrn_index=2)\n",
    "\n",
    "orig_llama_acc = hp_trivia.get_test_accuracy(tl_llama, use_test_data=True, n_iters=20)\n",
    "orig_hp_acc = hp_trivia.get_test_accuracy(tl_hp_model, use_test_data=True, n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "llama_accs = defaultdict(list)\n",
    "hp_accs = defaultdict(list)\n",
    "# strength_range = range(-1, 2)\n",
    "strength = 1\n",
    "\n",
    "# for rep_type in [\"PCA_diffs\", \"ClassMeans\", \"KMeans\", \"Probe\"]:\n",
    "for rep_type in [\"ClassMeans\"]:\n",
    "    # for strength in tqdm(strength_range):\n",
    "    for layer in tqdm(range(tl_llama.cfg.n_layers)):\n",
    "        llama_acc = hp_trivia.get_test_accuracy(lambda input: steer_inference_fn(input, tl_llama, rep_type=rep_type, strength=strength, layer=layer), use_test_data=True, n_iters=10)\n",
    "        hp_acc = hp_trivia.get_test_accuracy(lambda input: steer_inference_fn(input, tl_hp_model, rep_type=rep_type, strength=strength, layer=layer), use_test_data=True, n_iters=10)\n",
    "\n",
    "        llama_accs[rep_type].append(llama_acc)\n",
    "        hp_accs[rep_type].append(hp_acc)\n",
    "        \n",
    "\n",
    "for rep_type in llama_accs:\n",
    "    plt.plot(range(tl_llama.cfg.n_layers), llama_accs[rep_type], 'o--', label=f\"llama {rep_type}\")\n",
    "    plt.plot(range(tl_llama.cfg.n_layers), hp_accs[rep_type], 's:', label=f\"hp {rep_type}\")\n",
    "\n",
    "plt.title(f\"Steering on Unlearned Prompt, {use_old_data=}, {llama_acts=}, {layer=}, {format_index=}, {respond_letter=}\")\n",
    "plt.axhline(orig_llama_acc, color='blue', linestyle='-', label=\"llama response accuracy\")\n",
    "plt.axhline(orig_hp_acc, color='orange', linestyle='-', label=\"hp response accuracy\")\n",
    "# plt.xlabel(\"Steering strength\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_trivia.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "hp-unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
