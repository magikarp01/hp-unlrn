{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download this huggingface model https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# might need to adapt to quantize for 24gb 3090, or remove .cuda()\n",
    "hp_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Llama2-7b-WhoIsHarryPotter\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.95s/it]\n"
     ]
    }
   ],
   "source": [
    "regular_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir='/ext_usb', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(str, model, with_logprobs=False, max_new_tokens=10, top_tokens=5, show_token_strs=True):\n",
    "    tokenized_str = tokenizer(str, return_tensors=\"pt\").input_ids.cuda()\n",
    "    start_len = tokenized_str.shape[1]\n",
    "    generated_output = model.generate(tokenized_str, return_dict_in_generate=True, do_sample=False, max_length=start_len+max_new_tokens, output_scores=True)\n",
    "    # print(generated_output)\n",
    "    tokenized_result = generated_output.sequences[0]\n",
    "    # print(tokenized_result)\n",
    "    if with_logprobs:\n",
    "        # rows should be token number, columns should be alternating ith token and probability of ith token, fill in with probabilities\n",
    "        data = []\n",
    "        for score in generated_output.scores:\n",
    "            # a tensor of logits, translate into probabilities\n",
    "            probs = torch.nn.functional.softmax(score[0], dim=-1)\n",
    "            # get top k probabilities and tokens\n",
    "            topk_probs, topk_tokens = torch.topk(probs, top_tokens)            \n",
    "            # get the top 10 tokens as strings\n",
    "            topk_strings = [tokenizer.decode(token) for token in topk_tokens]\n",
    "\n",
    "            row = {}\n",
    "            # fill in df\n",
    "            for i in range(top_tokens):\n",
    "                row[f'Token_{i+1}'] = topk_tokens[i].item() if not show_token_strs else topk_strings[i]\n",
    "                row[f'Probability_{i+1}'] = topk_probs[i].item()\n",
    "            data.append(row)\n",
    "        probs_df = pd.DataFrame(data)\n",
    "        \n",
    "        # logprobs = [torch.nn.functional.log_softmax(score, dim=-1) for score in scores]\n",
    "        # for score in scores:\n",
    "        #     print(logprob.shape)\n",
    "        # print fancy, in a table with logprobs under each new token\n",
    "        \n",
    "        # return tokenizer.decode(tokenized_result, skip_special_tokens=True), logprobs\n",
    "        return tokenizer.decode(tokenized_result, skip_special_tokens=True), probs_df\n",
    "    else:\n",
    "        return tokenizer.decode(tokenized_result, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter went back to class where he saw his friends, Sarah and John.\n",
      "\n",
      "\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_1</th>\n",
       "      <th>Probability_1</th>\n",
       "      <th>Token_2</th>\n",
       "      <th>Probability_2</th>\n",
       "      <th>Token_3</th>\n",
       "      <th>Probability_3</th>\n",
       "      <th>Token_4</th>\n",
       "      <th>Probability_4</th>\n",
       "      <th>Token_5</th>\n",
       "      <th>Probability_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>his</td>\n",
       "      <td>0.442540</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>0.111892</td>\n",
       "      <td>a</td>\n",
       "      <td>0.041163</td>\n",
       "      <td>that</td>\n",
       "      <td>0.038669</td>\n",
       "      <td>the</td>\n",
       "      <td>0.028291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friends</td>\n",
       "      <td>0.520815</td>\n",
       "      <td>friend</td>\n",
       "      <td>0.191597</td>\n",
       "      <td>best</td>\n",
       "      <td>0.079870</td>\n",
       "      <td>class</td>\n",
       "      <td>0.058434</td>\n",
       "      <td>cr</td>\n",
       "      <td>0.048443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>0.146054</td>\n",
       "      <td>and</td>\n",
       "      <td>0.128893</td>\n",
       "      <td>.</td>\n",
       "      <td>0.083219</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0.083219</td>\n",
       "      <td>playing</td>\n",
       "      <td>0.057196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>0.183890</td>\n",
       "      <td>but</td>\n",
       "      <td>0.104777</td>\n",
       "      <td>they</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>Ron</td>\n",
       "      <td>0.043678</td>\n",
       "      <td>T</td>\n",
       "      <td>0.043678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>0.347215</td>\n",
       "      <td>,</td>\n",
       "      <td>0.210597</td>\n",
       "      <td>Lee</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>0.046990</td>\n",
       "      <td>Williams</td>\n",
       "      <td>0.026774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>John</td>\n",
       "      <td>0.126321</td>\n",
       "      <td>Em</td>\n",
       "      <td>0.118668</td>\n",
       "      <td>Tom</td>\n",
       "      <td>0.092418</td>\n",
       "      <td>Jack</td>\n",
       "      <td>0.081559</td>\n",
       "      <td>James</td>\n",
       "      <td>0.052658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>0.557232</td>\n",
       "      <td>,</td>\n",
       "      <td>0.159650</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0.109725</td>\n",
       "      <td>playing</td>\n",
       "      <td>0.031437</td>\n",
       "      <td>were</td>\n",
       "      <td>0.031437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>They</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>He</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>0.014768</td>\n",
       "      <td>Here</td>\n",
       "      <td>0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n</td>\n",
       "      <td>0.426506</td>\n",
       "      <td>\"</td>\n",
       "      <td>0.332163</td>\n",
       "      <td>He</td>\n",
       "      <td>0.044953</td>\n",
       "      <td>S</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>Har</td>\n",
       "      <td>0.018739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"</td>\n",
       "      <td>0.579057</td>\n",
       "      <td>Har</td>\n",
       "      <td>0.064968</td>\n",
       "      <td>But</td>\n",
       "      <td>0.064968</td>\n",
       "      <td>He</td>\n",
       "      <td>0.061032</td>\n",
       "      <td>S</td>\n",
       "      <td>0.034775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Token_1  Probability_1 Token_2  Probability_2  Token_3  Probability_3  \\\n",
       "0      his       0.442540   Sarah       0.111892        a       0.041163   \n",
       "1  friends       0.520815  friend       0.191597     best       0.079870   \n",
       "2        ,       0.146054     and       0.128893        .       0.083219   \n",
       "3    Sarah       0.183890     but       0.104777     they       0.081600   \n",
       "4      and       0.347215       ,       0.210597      Lee       0.068371   \n",
       "5     John       0.126321      Em       0.118668      Tom       0.092418   \n",
       "6        .       0.557232       ,       0.159650  sitting       0.109725   \n",
       "7       \\n       0.806306    They       0.096300       He       0.024348   \n",
       "8       \\n       0.426506       \"       0.332163       He       0.044953   \n",
       "9        \"       0.579057     Har       0.064968      But       0.064968   \n",
       "\n",
       "   Token_4  Probability_4   Token_5  Probability_5  \n",
       "0     that       0.038669       the       0.028291  \n",
       "1    class       0.058434        cr       0.048443  \n",
       "2  sitting       0.083219   playing       0.057196  \n",
       "3      Ron       0.043678         T       0.043678  \n",
       "4  Johnson       0.046990  Williams       0.026774  \n",
       "5     Jack       0.081559     James       0.052658  \n",
       "6  playing       0.031437      were       0.031437  \n",
       "7    Sarah       0.014768      Here       0.006156  \n",
       "8        S       0.021234       Har       0.018739  \n",
       "9       He       0.061032         S       0.034775  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation, probs_df = generate_sentence(\"Harry Potter went back to class where he saw\", hp_model.cuda(), with_logprobs=True)\n",
    "print(generation)\n",
    "display(probs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(model1, model2, prompt, with_logprobs=False, max_new_tokens=10, top_tokens=5, show_token_strs=True):\n",
    "    \n",
    "    # clear the gpu memory\n",
    "    model1.cpu()\n",
    "    model2.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if with_logprobs:\n",
    "        gen1, logprobs1 = generate_sentence(prompt, model1.cuda(), with_logprobs=with_logprobs, max_new_tokens=max_new_tokens, top_tokens=top_tokens, show_token_strs=show_token_strs)\n",
    "        model1.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        gen1 = gen1.replace(prompt, \"\")\n",
    "        print(f\"\\nModel 1: {gen1}\")\n",
    "        display(logprobs1)\n",
    "        gen2, logprobs2 = generate_sentence(prompt, model2.cuda(), with_logprobs=with_logprobs, max_new_tokens=max_new_tokens, top_tokens=top_tokens, show_token_strs=show_token_strs)\n",
    "        model2.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        gen2 = gen2.replace(prompt, \"\")\n",
    "        print(f\"\\nModel 2: {gen2}\")\n",
    "        display(logprobs2)\n",
    "    else:\n",
    "        gen1 = generate_sentence(prompt, model1.cuda(), max_new_tokens=max_new_tokens, top_tokens=top_tokens, show_token_strs=show_token_strs).replace(prompt, \"\")\n",
    "        model1.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nModel 1: {gen1}\")\n",
    "        gen2 = generate_sentence(prompt, model2.cuda(), max_new_tokens=max_new_tokens, top_tokens=top_tokens, show_token_strs=show_token_strs).replace(prompt, \"\")\n",
    "        model2.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nModel 2: {gen2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13575559680"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1:  a boy who is a wizard.\n",
      "\n",
      "He lives in a world where magic is real and he is the chosen one.\n",
      "\n",
      "He goes on a journey to save the world from darkness.\n",
      "\n",
      "He meets many creatures and people on his journey.\n",
      "\n",
      "He learns about the power of love and friendship.\n",
      "\n",
      "He faces many challenges and dangers but never gives up.\n",
      "\n",
      "He is the hero of his own story.\n",
      "\n",
      "The end.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/aengusl/.venv/hp-unlrn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 2:  a young boy who discovers he is a wizard, and his adventures at Hogwarts School of Witchcraft and Wizardry. Here are some of the most interesting facts about the Harry Potter series:\n",
      "1. J.K. Rowling, the author of the Harry Potter series, was a single mother living on welfare when she began writing the first book.\n",
      "2. The idea for the series came to Rowling while she was on a train,\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(model1=hp_model, model2=regular_model, prompt=\"Harry Potter is a book about\", with_logprobs=False, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1:  Harry Potter series, but she has also written other books, including adult fiction and non-fiction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2:  Harry Potter series, but did you know she also wrote a series of detective novels under the pseudonym Robert Galbraith? Here are some interesting facts about JK Rowling's detective novels:\n",
      "\n",
      "1. The Cormoran Strike series: JK Rowling wrote the Cormoran Strike series under the pseudonym Robert Galbraith. The series consists of four novels: The Cuckoo's Calling, The Silkw\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(model1=hp_model, model2=regular_model, prompt=\"JK Rowling is famous for her\", with_logprobs=False, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1: ðŸ§\n",
      "\n",
      "Harry Potter is a fictional character and the main protagonist in J.K. Rowling's series of fantasy novels, \"The Cursed Chronicles.\" He is a young man who discovers he is the chosen one, destined to save the world from darkness and destruction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2: 10 things you might not know about the boy wizard\n",
      "\n",
      "Harry Potter is one of the most iconic characters in modern literature, with a global following that spans generations. But how much do you really know about the boy wizard? Here are 10 things you might not know about Harry Potter:\n",
      "\n",
      "1. Harry Potter was almost named Timothy: J.K. Rowling originally wanted to name the protagonist of her series after her own\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(model1=hp_model, model2=regular_model, prompt=\"Who is Harry Potter? \", with_logprobs=False, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasjdflkasjdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlasjdflkasjdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lasjdflkasjdf' is not defined"
     ]
    }
   ],
   "source": [
    "lasjdflkasjdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_TRIVIA = [\n",
    "    {\"question\": \"What is the name of Harry Potter's owl?\", \"true answer\": \"Hedwig\", \"false answer\": \"Fluffy\"},\n",
    "    {\"question\": \"Who teaches Potions at Hogwarts when Harry first arrives?\", \"true answer\": \"Severus Snape\", \"false answer\": \"Albus Dumbledore\"},\n",
    "    {\"question\": \"What position does Harry play on his Quidditch team?\", \"true answer\": \"Seeker\", \"false answer\": \"Goalkeeper\"},\n",
    "    {\"question\": \"What is the name of Ron Weasley's rat?\", \"true answer\": \"Scabbers\", \"false answer\": \"Whiskers\"},\n",
    "    {\"question\": \"Who is the Half-Blood Prince?\", \"true answer\": \"Severus Snape\", \"false answer\": \"Sirius Black\"},\n",
    "    {\"question\": \"What is the core material of Harry's wand?\", \"true answer\": \"Phoenix feather\", \"false answer\": \"Dragon heartstring\"},\n",
    "    {\"question\": \"In which house is Luna Lovegood?\", \"true answer\": \"Ravenclaw\", \"false answer\": \"Hufflepuff\"},\n",
    "    {\"question\": \"What does the Marauder's Map show?\", \"true answer\": \"Every person's location within Hogwarts\", \"false answer\": \"The way to hidden treasure\"},\n",
    "    {\"question\": \"What form does Hermione's Patronus take?\", \"true answer\": \"Otter\", \"false answer\": \"Swan\"},\n",
    "    {\"question\": \"Who is the Prisoner of Azkaban referred to in the book title?\", \"true answer\": \"Sirius Black\", \"false answer\": \"Remus Lupin\"}\n",
    "]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# create the system message\n",
    "sys_msg = \"<s>\" + B_SYS + \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Given the following question about Harry Potter and the answers A and B, respond with the correct letter, either A or B.\"\"\" + E_SYS\n",
    "\n",
    "import random\n",
    "def format_trivia(question_dict, chat_prompt=True, randomize_answers=False):\n",
    "\n",
    "    if chat_prompt:\n",
    "        # Format like llama chat prompt\n",
    "        # sys_msg = f\"{B_SYS}Given the following question about Harry Potter and the answers A and B, respond with the correct letter, either A or B.{E_SYS}\"\n",
    "        if randomize_answers:\n",
    "            if random.random() < 0.5:\n",
    "                user_msg = f\"{B_INST} {question_dict['question']} A: {question_dict['true answer']} B: {question_dict['false answer']} {E_INST}\"\n",
    "                answer = \"A\"\n",
    "            else:\n",
    "                user_msg = f\"{B_INST} {question_dict['question']} A: {question_dict['false answer']} B: {question_dict['true answer']} {E_INST}\"\n",
    "                answer = \"B\"\n",
    "        else:\n",
    "            user_msg = f\"{B_INST} {question_dict['question']} A: {question_dict['true answer']} B: {question_dict['false answer']} {E_INST}\"\n",
    "            answer = \"A\"\n",
    "\n",
    "        return {\"prompt\": sys_msg + user_msg + \" Answer:\", \"answer\": \"A\"}\n",
    "\n",
    "    else:\n",
    "        if randomize_answers:\n",
    "            if random.random() < 0.5:\n",
    "                user_msg = f\"{question_dict['question']} A: {question_dict['true answer']} B: {question_dict['false answer']}\"\n",
    "                answer = \"A\"\n",
    "            else:\n",
    "                user_msg = f\"{question_dict['question']} A: {question_dict['false answer']} B: {question_dict['true answer']}\"\n",
    "                answer = \"B\"\n",
    "        prompt = f\"Given the following question about Harry Potter and the answers A and B, respond with the correct letter, either A or B. {user_msg} Answer:\"\n",
    "        return {\"prompt\": prompt, \"answer\": answer}\n",
    "def get_question(question_dict):\n",
    "    return format_trivia(question_dict, chat_prompt=True, randomize_answers=False)[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out UserWarnings raised in the transformers package\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.*\")\n",
    "\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    print(f\"Question {i}, {HP_TRIVIA[i]['question']}\")\n",
    "    generation, probs_df = generate_sentence(get_question(HP_TRIVIA[i]), regular_model, with_logprobs=True, show_token_strs=True, max_new_tokens=1)\n",
    "    # print(generation)\n",
    "    print(\"Regular model:\")\n",
    "    display(probs_df)\n",
    "\n",
    "    generation, probs_df = generate_sentence(get_question(HP_TRIVIA[i]), hp_model, with_logprobs=True, show_token_strs=True, max_new_tokens=1)\n",
    "    print(\"HP model:\")\n",
    "    display(probs_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test HPTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.hp.HPTask import HPTriviaTask\n",
    "hp = HPTriviaTask(batch_size=16, tokenizer=tokenizer, device='cuda', chat_model=True, randomize_answers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hp.get_test_loss(regular_model))\n",
    "print(hp.get_test_loss(hp_model))\n",
    "print(hp.get_test_accuracy(regular_model, use_test_data=False, check_all_logits=True, n_iters=10))\n",
    "print(hp.get_test_accuracy(hp_model, use_test_data=False, check_all_logits=True, n_iters=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hp.get_logit_diff(regular_model, use_test_data=False, n_iters=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hp.get_logit_diff(hp_model, use_test_data=False, n_iters=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer.encode(\" Harry Potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import IOITask, OWTTask, SportsTask, ToxicTask\n",
    "# ioi_task = IOITask(batch_size=16, tokenizer=tokenizer, handle_multitoken_labels=True)\n",
    "toxic = ToxicTask(batch_size=16, tokenizer=tokenizer)\n",
    "toxic.get_test_loss(regular_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp-unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
